# -*- coding: utf-8 -*-
"""startup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_DORjZj_PYMZ2mzunIv0clC17SsPT3fp
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.graphics.regressionplots import influence_plot
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

df = pd.read_csv('/content/50_Startups.csv')
df

df.info()

df.head()

df.tail()

df.describe()

df.isnull().sum()

df.duplicated().sum()

df.corr(numeric_only=True)

df=df.drop(df.index[[40,49]],axis=0).reset_index()
df

# Create the scatterplot matrix with histograms
sns.pairplot(df)

# Show the plot
plt.show()

#heatmap
sns.heatmap(df.corr(numeric_only=True),annot=True)
plt.show()

#boxplot
sns.boxplot(data=df)
plt.show()

#hist
df.hist(figsize=(10,10))
plt.show()

#violin
sns.violinplot(data=df)
plt.show()

#Outlier Handling
outliers = []
def detect_outliers_zscore(data):
    thres = 3
    mean = np.mean(data)
    std = np.std(data)
    print(mean, std) # Print mean and std inside the function

# Call the function with your data to calculate and print mean, std
detect_outliers_zscore(df['Profit']) # Example: Assuming you want to analyze 'Profit' column

#Preparing a model
import statsmodels.formula.api as smf
model=smf.ols('Profit~Administration+Q("Marketing Spend")+C(State)',data=df).fit()

#Coefficients
model.params

#t and p-Values
print(model.tvalues, '\n', model.pvalues)

#R squared values
(model.rsquared,model.rsquared_adj)

# Simple Linear Regression Models
ml_R = smf.ols('Profit~Q("Marketing Spend")',data = df).fit()
sl_R = smf.ols('Profit~Q("R&D Spend")',data = df).fit()
ml_M = smf.ols('Profit~Administration+Q("Marketing Spend")',data = df).fit()
ml_A = smf.ols('Profit~Administration',data = df).fit()
for m in [ml_R,sl_R,ml_M,ml_A]:
    print(m.rsquared)

#calculating vif
rsq_r = smf.ols('Profit~Q("R&D Spend")',data=df).fit().rsquared
vif_r = 1/(1-rsq_r)
print('VIF for R&D Spend is',vif_r)

rsq_m = smf.ols('Profit~Q("Marketing Spend")',data=df).fit().rsquared
vif_m = 1/(1-rsq_m)
print('VIF for Marketing Spend is',vif_m)

rsq_A= smf.ols('Profit~Administration',data=df).fit().rsquared
vif_A = 1/(1-rsq_A)
print('VIF for Administration is',vif_A)

# Storing vif values in a data frame
d1 = {'Variables':['Q("R&D Spend")','Q("Marketing Spend")','Administration'],'VIF':[vif_r,vif_m,vif_A]}
Vif_frame = pd.DataFrame(d1)
Vif_frame

# Residual Analysis
## Test for Normality of Residuals (Q-Q Plot)
import statsmodels.api as sm
qqplot=sm.qqplot(model.resid,line='q') # line = 45 to draw the diagnoal line
plt.title("Normal Q-Q plot of residuals")
plt.show()

# Assuming 'model' is your fitted OLS model
# Get the residuals
residuals = model.resid

# Create a DataFrame with the residuals
residuals_df = pd.DataFrame({'Residuals': residuals})
plt.scatter(residuals_df.index, residuals_df['Residuals'])
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Observation')
plt.ylabel('Residuals')
plt.title('Residuals vs. Observation Plot')
plt.show()

list(np.where(model.resid>10))

## Residual Plot for Homoscedasticity
def get_standardized_values(vals):
    return (vals-vals.mean())/vals.std()

plt.scatter(get_standardized_values(model.fittedvalues),
            get_standardized_values(model.resid))
plt.title('Residual Plot')
plt.xlabel('Standardized Fitted values')
plt.ylabel('Standardized residual values')
plt.show()

import statsmodels.api as sm
fig = plt.figure(figsize=(12, 8))  # Create the figure first
exog_name='R&D Spend' # Changed from 'Q("R&D Spend")' to 'R&D Spend'
exog = sm.add_constant(df[exog_name])
model = sm.OLS(df['Profit'], exog).fit()
fig = sm.graphics.plot_regress_exog(model, exog_name, fig=fig)
plt.show()

## Detecting Influencers/Outliers
# prompt: ## Detecting Influencers/Outliers
# cook's distance
model_influence = model.get_influence()
(c, _) = model_influence.cooks_distance
plt.stem(np.arange(len(df)), np.round(c, 3))
plt.xlabel('Row index')
plt.ylabel('Cooks Distance')
plt.show()

#index and value of influencer where c is more than .5
np.argmax(c) , np.max(c)

## High Influence points
df[df.index.isin([26])]

#Splitting the dataset into the Training set and Test set¶
x=df.drop(columns=['Profit','State'],axis=1)
y=df['Profit']
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)

from sklearn.linear_model import LinearRegression
regressor=LinearRegression()
regressor.fit(x_train,y_train)

#Predicting the Test set results

y_pred=regressor.predict(x_test)
y_pred

#R2 SCORE FUNCTION¶
r2_score(y_test,y_pred)

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

from sklearn.neighbors import KNeighborsRegressor # Import KNeighborsRegressor

knn = KNeighborsRegressor(n_neighbors=5) # Change to KNeighborsRegressor
knn.fit(x_train, y_train)

# Make predictions on the test data
y_pred = knn.predict(x_test)

r2_score(y_test,y_pred)
mse=mean_squared_error(y_test,y_pred)
print('MSE:',mse)
print('RMSE:',np.sqrt(mse))

#plot
plt.scatter(y_test,y_pred)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show()